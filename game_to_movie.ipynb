{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"game_to_movie.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"cuPlX3pB8WYn"},"source":["# Game to Movie Video Conversion"]},{"cell_type":"markdown","metadata":{"id":"ph-iaYTZ8WYr"},"source":["## Main Imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQemvyJ_8WYs","executionInfo":{"status":"ok","timestamp":1621264940481,"user_tz":-60,"elapsed":27264,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}},"outputId":"12a4a4cc-979a-4633-998f-9038229bcc50"},"source":["from os import listdir\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import cv2\n","import random\n","import natsort\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","from keras.optimizers import Adam\n","from keras.initializers import RandomNormal\n","from keras.models import Model\n","from keras.models import Input\n","from keras.layers import Conv2D\n","from keras.layers import Conv2DTranspose\n","from keras.layers import LeakyReLU\n","from keras.layers import Activation\n","from keras.layers import Concatenate\n","from keras.preprocessing.image import load_img, img_to_array\n","!pip install git+https://www.github.com/keras-team/keras-contrib.git\n","from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","Collecting git+https://www.github.com/keras-team/keras-contrib.git\n","  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-xhh7gudm\n","  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-xhh7gudm\n","Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.4.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-contrib==2.0.8) (1.15.0)\n","Building wheels for collected packages: keras-contrib\n","  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101065 sha256=49567335f1ed6833592342c56b146a76bae52a46b2e860a0fc3f9e4d868a55eb\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-56thsijz/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n","Successfully built keras-contrib\n","Installing collected packages: keras-contrib\n","Successfully installed keras-contrib-2.0.8\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9RA6ozwm8WYs"},"source":["## Preprocess the video data\n","### Function to read frames from files"]},{"cell_type":"code","metadata":{"id":"90f0x4uj8WYt"},"source":["def read_frames(video_path, save_path, problem_frames, start_frame, end_frame, frame_skip, counter):\n","    \n","    # initialise frame capturing\n","    vidcap = cv2.VideoCapture(video_path)\n","    vidcap.set(1, start_frame)\n","    success, image = vidcap.read()\n","    frame_number = start_frame\n","    \n","    while success and frame_number < end_frame:\n","\n","        # save frames as images, capturing every frame_skip frames\n","        cv2.imwrite(save_path + '%d.jpg' % counter, image)\n","        frame_number += frame_skip\n","        counter += 1\n","        \n","        if frame_number in problem_frames:\n","            original_frame_number = frame_number\n","            random_frame = random.randint(start_frame, end_frame)\n","            vidcap.set(1, random_frame)\n","            frame_number = original_frame_number\n","        else:\n","            vidcap.set(1, frame_number)\n","        success, image = vidcap.read()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gefJW2Yv8WYt"},"source":["### Initialise file paths"]},{"cell_type":"code","metadata":{"id":"dXzReM3Y8WYt","executionInfo":{"status":"ok","timestamp":1621264946002,"user_tz":-60,"elapsed":452,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}}},"source":["main_path = '/content/drive/My Drive/University Work/Year 4/Advanced Computer Vision/'\n","game_load_path = main_path + 'data/game/MafiaVideogame.mp4'\n","game_save_path = main_path + 'data/game/frames/'\n","game_problem_frames = [6510, 6698, 11774, 11962, 15158, 20798, 22490, 23054, 29070, 34334, 34522, 41290, 41478, 49562, 49750, \n","                       65730, 65918, 66106, 69302, 73438, 73626, 74002, 80018, 80206, 87162, 89230, 92426, 96938, 103518,\n","                      103706, 103894, 116302, 130590, 130778, 138298, 138486, 142058, 147510, 147698, 154654, 154842, 157662,\n","                      166686, 166874, 173642, 173830, 180410, 192066, 192254, 196578, 202406, 202594, 207106, 214626, 221582,\n","                      221770, 228914, 229102]\n","\n","movies_folder = main_path + 'data/movie/'\n","movie_save_path = main_path + 'data/movie/frames/'\n","\n","# data holding start frame, end frame, frame skip\n","movie_data = {\n","    \"TheGodfather.mp4\": (204, 12204, 30, [1074, 1104, 1134, 1164, 1194]),\n","    \"TheIrishman.mp4\": (75, 23275, 58, [12603, 12661, 12719]),\n","    \"TheSopranos.mp4\": (80, 51280, 128, [9424, 11472, 27984, 28112, 28240, 48848])\n","}"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vdxwtpw58WYu"},"source":["### Read frames from game file"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eYcFRqEf8WYu","executionInfo":{"status":"ok","timestamp":1619454840872,"user_tz":-60,"elapsed":252709,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}},"outputId":"25c815cb-7f2d-4795-aba9-3fcd25044da7"},"source":["read_frames(game_load_path, game_save_path, game_problem_frames, 6510, 232110, 188, counter=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/University Work/Year 4/Advanced Computer Vision/data/game/MafiaVideogame.mp4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HzU3KDAc8WYu"},"source":["### Read frames from movie files"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vptjHEqc8WYu","executionInfo":{"status":"ok","timestamp":1619455328672,"user_tz":-60,"elapsed":259076,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}},"outputId":"bb508259-94e0-42a7-e10e-92409fa54325"},"source":["counter = 1\n","for movie_name, frame_data in movie_data.items():\n","    \n","    movie_load_path = movies_folder + movie_name\n","    start_frame, end_frame, frame_skip, movie_problem_frames = frame_data[0], frame_data[1], frame_data[2], frame_data[3]\n","    read_frames(movie_load_path, movie_save_path, movie_problem_frames, start_frame, end_frame, frame_skip, counter)\n","    counter += 400"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/University Work/Year 4/Advanced Computer Vision/data/movie/TheGodfather.mp4\n","/content/drive/My Drive/University Work/Year 4/Advanced Computer Vision/data/movie/TheIrishman.mp4\n","/content/drive/My Drive/University Work/Year 4/Advanced Computer Vision/data/movie/TheSopranos.mp4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Na0p5IAa8WYv"},"source":["### Set train/validation/test sizes and paths"]},{"cell_type":"code","metadata":{"id":"qeW6t-NY8WYv","executionInfo":{"status":"ok","timestamp":1621264976018,"user_tz":-60,"elapsed":1137,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}}},"source":["game_train_path = main_path + 'data/game_train/'\n","game_val_path = main_path + 'data/game_val/'\n","game_test_path = main_path + 'data/game_test/'\n","\n","movie_train_path = main_path + 'data/movie_train/'\n","movie_val_path = main_path + 'data/movie_val/'\n","movie_test_path = main_path + 'data/movie_test/'\n","\n","data_length = len(listdir(game_save_path))\n","train_size = int(0.8 * data_length)\n","val_size = int(0.2 * train_size)\n","train_size -= val_size\n","test_size = int(0.2 * data_length)\n","\n","game_data = natsort.natsorted(listdir(game_save_path))\n","movie_data = natsort.natsorted(listdir(movie_save_path))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5sinnH5H8WYw"},"source":["### Function to split the data into folders"]},{"cell_type":"code","metadata":{"id":"wCedOWDe8WYw"},"source":["def split_data(size, data, save_path, new_save_path):\n","    \n","    for i in range(size[0], size[1]):\n","\n","        image_name = data[i]\n","        image_path = save_path + image_name\n","\n","        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n","        cv2.imwrite(new_save_path + image_name, image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-qJaY7ck8WYw"},"source":["### Splitting the data"]},{"cell_type":"code","metadata":{"id":"E9x22Tti8WYw"},"source":["# split game data\n","split_data((0, train_size), game_data, game_save_path, game_train_path)\n","split_data((train_size, train_size + val_size), game_data, game_save_path, game_val_path)\n","split_data((train_size + val_size, data_length), game_data, game_save_path, game_test_path)\n","\n","# split movie data\n","split_data((0, train_size), movie_data, movie_save_path, movie_train_path)\n","split_data((train_size, train_size + val_size), movie_data, movie_save_path, movie_val_path)\n","split_data((train_size + val_size, data_length), movie_data, movie_save_path, movie_test_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z8FiIHN68WYw","executionInfo":{"status":"ok","timestamp":1621264985070,"user_tz":-60,"elapsed":424,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}}},"source":["def load_images(path, size=(256, 256)):\n","    \n","    data_list = list()\n","    \n","    for filename in listdir(path):\n","        \n","        # load and resize the image\n","        pixels = load_img(path + filename, target_size=size)\n","        pixels = img_to_array(pixels)\n","        data_list.append(pixels)\n","    \n","    return np.asarray(data_list)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2oFDfwKA8WYx","executionInfo":{"status":"ok","timestamp":1621265486301,"user_tz":-60,"elapsed":500117,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}},"outputId":"1b4073c6-7ae2-424f-a918-77b1e37e77e9"},"source":["# load dataset A\n","game_data = load_images(game_train_path)\n","print('Loaded game data:', game_data.shape)\n","\n","# load dataset B\n","movie_data = load_images(movie_train_path)\n","print('Loaded movie data:', movie_data.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Loaded game data: (768, 256, 256, 3)\n","Loaded movie data: (768, 256, 256, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h-wfcEks8WYx","executionInfo":{"status":"ok","timestamp":1621265509667,"user_tz":-60,"elapsed":1602,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}}},"source":["game_data = (game_data - 127.5) / 127.5\n","movie_data = (movie_data - 127.5) / 127.5"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U5vm9hw_ITDJ"},"source":["## CycleGAN Model"]},{"cell_type":"markdown","metadata":{"id":"Dc72ecwzI3Ir"},"source":["### Discriminator"]},{"cell_type":"code","metadata":{"id":"BkoUqn1kHu1C","executionInfo":{"status":"ok","timestamp":1621269015193,"user_tz":-60,"elapsed":778,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}}},"source":["def define_discriminator(image_shape):\n","    # weight initialization\n","    init = RandomNormal(stddev=0.02)\n","    # source image input\n","    in_image = Input(shape=image_shape)\n","    # C64\n","    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    # C128\n","    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","    d = InstanceNormalization(axis=-1)(d)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    # C256\n","    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","    d = InstanceNormalization(axis=-1)(d)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    # C512\n","    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","    d = InstanceNormalization(axis=-1)(d)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    # second last output layer\n","    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n","    d = InstanceNormalization(axis=-1)(d)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    # patch output\n","    patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n","    # define model\n","    model = Model(in_image, patch_out)\n","    # compile model\n","    model.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5])\n","    return model"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_M-yzHYcI7rL"},"source":["### Resnet Block"]},{"cell_type":"code","metadata":{"id":"Ox3QYEUEHxGa","executionInfo":{"status":"ok","timestamp":1621269017414,"user_tz":-60,"elapsed":1015,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}}},"source":["def resnet_block(n_filters, input_layer):\n","    # weight initialization\n","    init = RandomNormal(stddev=0.02)\n","    # first layer convolutional layer\n","    g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n","    g = InstanceNormalization(axis=-1)(g)\n","    g = Activation('relu')(g)\n","    # second convolutional layer\n","    g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n","    g = InstanceNormalization(axis=-1)(g)\n","    # concatenate merge channel-wise with input layer\n","    g = Concatenate()([g, input_layer])\n","    return g"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VyT82jOYJGvr"},"source":["### Generator"]},{"cell_type":"code","metadata":{"id":"FtkwL5TDH0yJ","executionInfo":{"status":"ok","timestamp":1621269019688,"user_tz":-60,"elapsed":426,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}}},"source":["# define the standalone generator model\n","def define_generator(image_shape, n_resnet=9):\n","    # weight initialization\n","    init = RandomNormal(stddev=0.02)\n","    # image input\n","    in_image = Input(shape=image_shape)\n","    # c7s1-64\n","    g = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n","    g = InstanceNormalization(axis=-1)(g)\n","    g = Activation('relu')(g)\n","    # d128\n","    g = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n","    g = InstanceNormalization(axis=-1)(g)\n","    g = Activation('relu')(g)\n","    # d256\n","    g = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n","    g = InstanceNormalization(axis=-1)(g)\n","    g = Activation('relu')(g)\n","    # R256\n","    for _ in range(n_resnet):\n","        g = resnet_block(256, g)\n","    # u128\n","    g = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n","    g = InstanceNormalization(axis=-1)(g)\n","    g = Activation('relu')(g)\n","    # u64\n","    g = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n","    g = InstanceNormalization(axis=-1)(g)\n","    g = Activation('relu')(g)\n","    # c7s1-3\n","    g = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n","    g = InstanceNormalization(axis=-1)(g)\n","    out_image = Activation('tanh')(g)\n","    # define model\n","    model = Model(in_image, out_image)\n","    return model"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dOuHCtxRJSoc"},"source":["### Composite Model for Generator Training with Adversarial and Cycle loss"]},{"cell_type":"code","metadata":{"id":"snJ58oHYH36p","executionInfo":{"status":"ok","timestamp":1621269021637,"user_tz":-60,"elapsed":502,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}}},"source":["def define_composite_model(g_model_1, d_model, g_model_2, image_shape):\n","    # ensure the model we're updating is trainable\n","    g_model_1.trainable = True\n","    # mark discriminator as not trainable\n","    d_model.trainable = False\n","    # mark other generator model as not trainable\n","    g_model_2.trainable = False\n","    # discriminator element\n","    input_gen = Input(shape=image_shape)\n","    gen1_out = g_model_1(input_gen)\n","    output_d = d_model(gen1_out)\n","    # identity element\n","    input_id = Input(shape=image_shape)\n","    output_id = g_model_1(input_id)\n","    # forward cycle\n","    output_f = g_model_2(gen1_out)\n","    # backward cycle\n","    gen2_out = g_model_2(input_id)\n","    output_b = g_model_1(gen2_out)\n","    # define model graph\n","    model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n","    # define optimization algorithm configuration\n","    opt = Adam(lr=0.0002, beta_1=0.5)\n","    # compile model with weighting of least squares loss and L1 loss\n","    model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=opt)\n","    return model"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FLwVZHlTJc7l"},"source":["### Load and prepare training images"]},{"cell_type":"code","metadata":{"id":"xGosQBleIDkp","executionInfo":{"status":"ok","timestamp":1621269023738,"user_tz":-60,"elapsed":476,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}}},"source":["def load_real_samples(filename):\n","    # load the dataset\n","    data = np.load(filename)\n","    # unpack arrays\n","    X1, X2 = data['arr_0'], data['arr_1']\n","    # scale from [0,255] to [-1,1]\n","    X1 = (X1 - 127.5) / 127.5\n","    X2 = (X2 - 127.5) / 127.5\n","    return [X1, X2]"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4N7FcXApJiPc"},"source":["### Select batch of random samples and return images and target"]},{"cell_type":"code","metadata":{"id":"k2TrPpXbIFFq","executionInfo":{"status":"ok","timestamp":1621269025094,"user_tz":-60,"elapsed":447,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}}},"source":["def generate_real_samples(dataset, n_samples, patch_shape):\n","    # choose random instances\n","    ix = np.random.randint(0, dataset.shape[0], n_samples)\n","    # retrieve selected images\n","    X = dataset[ix]\n","    # generate 'real' class labels (1)\n","    y = np.ones((n_samples, patch_shape, patch_shape, 1))\n","    return X, y"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1lFe9dStJmcm"},"source":["### Generate batch of images and return images and targets"]},{"cell_type":"code","metadata":{"id":"z9uVoQYsIIHJ","executionInfo":{"status":"ok","timestamp":1621269026499,"user_tz":-60,"elapsed":459,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}}},"source":["def generate_fake_samples(g_model, dataset, patch_shape):\n","    # generate fake instance\n","    X = g_model.predict(dataset)\n","    # create 'fake' class labels (0)\n","    y = np.zeros((len(X), patch_shape, patch_shape, 1))\n","    return X, y"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xoWCAZS9Jwr3"},"source":["### Save the generators"]},{"cell_type":"code","metadata":{"id":"y2wMOL3uIJ_C","executionInfo":{"status":"ok","timestamp":1621269028210,"user_tz":-60,"elapsed":788,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}}},"source":["def save_models(step, g_model_AtoB, g_model_BtoA):\n","    # save the first generator model\n","    filename1 = 'g_model_AtoB_%06d.h5' % (step+1)\n","    g_model_AtoB.save(filename1)\n","    # save the second generator model\n","    filename2 = 'g_model_BtoA_%06d.h5' % (step+1)\n","    g_model_BtoA.save(filename2)\n","    print('>Saved: %s and %s' % (filename1, filename2))"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"06UEqnnXJ4EW"},"source":["### Plot a summary of the performance"]},{"cell_type":"code","metadata":{"id":"AcqPCZjfIN7p","executionInfo":{"status":"ok","timestamp":1621269029693,"user_tz":-60,"elapsed":574,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}}},"source":["def summarize_performance(step, g_model, trainX, name, n_samples=5):\n","    # select a sample of input images\n","    X_in, _ = generate_real_samples(trainX, n_samples, 0)\n","    # generate translated images\n","    X_out, _ = generate_fake_samples(g_model, X_in, 0)\n","    # scale all pixels from [-1,1] to [0,1]\n","    X_in = (X_in + 1) / 2.0\n","    X_out = (X_out + 1) / 2.0\n","    # plot real images\n","    for i in range(n_samples):\n","        plt.subplot(2, n_samples, 1 + i)\n","        plt.axis('off')\n","        plt.imshow(X_in[i])\n","    # plot translated image\n","    for i in range(n_samples):\n","        plt.subplot(2, n_samples, 1 + n_samples + i)\n","        plt.axis('off')\n","        plt.imshow(X_out[i])\n","    # save plot to file\n","    filename1 = '%s_generated_plot_%06d.png' % (name, (step+1))\n","    plt.savefig(filename1)\n","    plt.close()"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RMkPD1elJ8lY"},"source":["### Update the image pool for fake images"]},{"cell_type":"code","metadata":{"id":"G0kW1B_AIQRa","executionInfo":{"status":"ok","timestamp":1621269031370,"user_tz":-60,"elapsed":467,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}}},"source":["def update_image_pool(pool, images, max_size=50):\n","    selected = list()\n","    for image in images:\n","        if len(pool) < max_size:\n","            # stock the pool\n","            pool.append(image)\n","            selected.append(image)\n","        elif random.random() < 0.5:\n","            # use image, but don't add it to the pool\n","            selected.append(image)\n","        else:\n","            # replace an existing image and use replaced image\n","            ix = np.random.randint(0, len(pool))\n","            selected.append(pool[ix])\n","            pool[ix] = image\n","    return np.asarray(selected)"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TuvvGtMyKZJH"},"source":["### Training loop"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jVV3FPPf8WYx","executionInfo":{"status":"error","timestamp":1621269264720,"user_tz":-60,"elapsed":232103,"user":{"displayName":"Faiz Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPZ2wigvvlNsGJdPqeQIQVxr-MyUl09zALC2dCQg=s64","userId":"11900101326516043479"}},"outputId":"7bf228db-57bb-4dee-d3ed-c69cbd9917cb"},"source":["def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset):\n","    # define properties of the training run\n","    n_epochs, n_batch, = 100, 4\n","    # determine the output square shape of the discriminator\n","    n_patch = d_model_A.output_shape[1]\n","    # unpack dataset\n","    trainA, trainB = dataset\n","    # prepare image pool for fakes\n","    poolA, poolB = list(), list()\n","    # calculate the number of batches per training epoch\n","    bat_per_epo = int(len(trainA) / n_batch)\n","    # calculate the number of training iterations\n","    n_steps = bat_per_epo * n_epochs\n","    # manually enumerate epochs\n","\n","    print(n_epochs, n_batch, bat_per_epo, n_steps)\n","\n","    for i in range(n_steps):\n","        # select a batch of real samples\n","        X_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)\n","        X_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)\n","        # generate a batch of fake samples\n","        X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n","        X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n","        # update fakes from pool\n","        X_fakeA = update_image_pool(poolA, X_fakeA)\n","        X_fakeB = update_image_pool(poolB, X_fakeB)\n","        # update generator B->A via adversarial and cycle loss\n","        g_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n","        # update discriminator for A -> [real/fake]\n","        dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n","        dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n","        # update generator A->B via adversarial and cycle loss\n","        g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n","        # update discriminator for B -> [real/fake]\n","        dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n","        dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n","        # summarize performance\n","        print('>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n","        # evaluate the model performance every so often\n","        if (i+1) % (bat_per_epo * 1) == 0:\n","            # plot A->B translation\n","            summarize_performance(i, g_model_AtoB, trainA, 'AtoB')\n","            # plot B->A translation\n","            summarize_performance(i, g_model_BtoA, trainB, 'BtoA')\n","        if (i+1) % (bat_per_epo * 5) == 0:\n","            # save the models\n","            save_models(i, g_model_AtoB, g_model_BtoA)\n","            \n","\n","# load image data\n","dataset = [game_data, movie_data]\n","print('Loaded', dataset[0].shape, dataset[1].shape)\n","# define input shape based on the loaded dataset\n","image_shape = dataset[0].shape[1:]\n","# generator: A -> B\n","g_model_AtoB = define_generator(image_shape)\n","# generator: B -> A\n","g_model_BtoA = define_generator(image_shape)\n","# discriminator: A -> [real/fake]\n","d_model_A = define_discriminator(image_shape)\n","# discriminator: B -> [real/fake]\n","d_model_B = define_discriminator(image_shape)\n","# composite: A -> B -> [real/fake, A]\n","c_model_AtoB = define_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape)\n","# composite: B -> A -> [real/fake, B]\n","c_model_BtoA = define_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape)\n","# train models\n","train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Loaded (768, 256, 256, 3) (768, 256, 256, 3)\n","100 4 192 19200\n","WARNING:tensorflow:5 out of the last 259 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3632a1fc20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:5 out of the last 763 calls to <function Model.make_train_function.<locals>.train_function at 0x7f3638b9c290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",">1, dA[1.710,3.988] dB[0.845,1.791] g[19.347,21.242]\n",">2, dA[3.074,2.946] dB[1.974,5.213] g[20.175,21.610]\n",">3, dA[4.277,1.723] dB[1.829,1.334] g[20.139,21.289]\n",">4, dA[2.119,1.300] dB[1.921,0.974] g[20.952,18.017]\n",">5, dA[1.761,1.337] dB[3.954,0.991] g[20.958,16.935]\n",">6, dA[0.608,1.838] dB[2.468,2.024] g[17.468,18.106]\n",">7, dA[0.485,1.248] dB[1.336,2.562] g[16.635,16.031]\n",">8, dA[0.482,1.765] dB[0.824,2.205] g[15.876,15.759]\n",">9, dA[0.391,0.985] dB[0.574,1.022] g[16.691,16.774]\n",">10, dA[0.812,0.909] dB[0.987,0.503] g[15.135,15.512]\n",">11, dA[0.688,1.331] dB[0.599,0.354] g[14.913,16.352]\n",">12, dA[0.874,0.906] dB[0.434,0.350] g[14.279,15.922]\n",">13, dA[0.876,0.459] dB[0.372,0.261] g[13.657,14.201]\n",">14, dA[0.676,0.306] dB[0.223,0.236] g[14.064,14.532]\n",">15, dA[0.427,0.275] dB[0.204,0.229] g[14.228,14.362]\n",">16, dA[0.330,0.246] dB[0.213,0.206] g[14.544,14.155]\n",">17, dA[0.294,0.316] dB[0.248,0.200] g[13.447,13.746]\n",">18, dA[0.256,0.349] dB[0.208,0.221] g[14.819,14.734]\n",">19, dA[0.369,0.258] dB[0.247,0.175] g[12.600,12.046]\n",">20, dA[0.216,0.259] dB[0.211,0.215] g[14.311,13.935]\n",">21, dA[0.272,0.214] dB[0.203,0.216] g[14.326,13.892]\n",">22, dA[0.221,0.232] dB[0.312,0.211] g[11.690,12.257]\n",">23, dA[0.226,0.201] dB[0.125,0.212] g[14.972,15.242]\n",">24, dA[0.232,0.215] dB[0.181,0.163] g[13.101,13.006]\n",">25, dA[0.214,0.279] dB[0.183,0.166] g[13.318,12.965]\n",">26, dA[0.195,0.277] dB[0.200,0.163] g[13.026,13.264]\n",">27, dA[0.284,0.218] dB[0.216,0.165] g[11.810,12.261]\n",">28, dA[0.229,0.227] dB[0.169,0.217] g[10.958,11.651]\n",">29, dA[0.192,0.183] dB[0.159,0.172] g[13.966,14.290]\n",">30, dA[0.249,0.170] dB[0.154,0.145] g[12.822,12.497]\n",">31, dA[0.165,0.139] dB[0.220,0.147] g[11.871,12.983]\n",">32, dA[0.172,0.184] dB[0.179,0.176] g[11.230,12.163]\n",">33, dA[0.256,0.196] dB[0.201,0.160] g[12.130,12.423]\n",">34, dA[0.203,0.136] dB[0.121,0.121] g[16.125,16.405]\n",">35, dA[0.345,0.189] dB[0.220,0.115] g[11.690,11.833]\n",">36, dA[0.261,0.186] dB[0.132,0.122] g[13.408,13.781]\n",">37, dA[0.242,0.138] dB[0.174,0.175] g[12.092,12.132]\n",">38, dA[0.176,0.172] dB[0.189,0.172] g[11.918,12.334]\n",">39, dA[0.254,0.158] dB[0.130,0.113] g[13.088,12.169]\n",">40, dA[0.095,0.118] dB[0.196,0.127] g[12.956,13.922]\n",">41, dA[0.325,0.164] dB[0.142,0.109] g[13.104,12.107]\n",">42, dA[0.186,0.147] dB[0.191,0.106] g[12.926,12.465]\n",">43, dA[0.250,0.203] dB[0.197,0.098] g[12.004,11.750]\n",">44, dA[0.139,0.082] dB[0.102,0.110] g[13.344,13.111]\n",">45, dA[0.168,0.129] dB[0.110,0.086] g[12.260,13.236]\n",">46, dA[0.247,0.180] dB[0.104,0.101] g[11.313,11.494]\n",">47, dA[0.225,0.197] dB[0.079,0.083] g[11.221,10.882]\n",">48, dA[0.117,0.149] dB[0.120,0.153] g[13.626,14.277]\n",">49, dA[0.243,0.135] dB[0.126,0.067] g[13.505,13.032]\n",">50, dA[0.190,0.145] dB[0.132,0.073] g[13.175,11.918]\n",">51, dA[0.151,0.071] dB[0.128,0.107] g[12.501,13.129]\n",">52, dA[0.154,0.088] dB[0.210,0.056] g[13.568,13.707]\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-aa8d4f266d22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mc_model_BtoA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_composite_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model_BtoA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_model_AtoB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# train models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_model_AtoB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_model_BtoA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_model_AtoB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_model_BtoA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-26-aa8d4f266d22>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mX_fakeB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_image_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoolB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_fakeB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# update generator B->A via adversarial and cycle loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mg_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mc_model_BtoA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_realB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# update discriminator for A -> [real/fake]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mdA_loss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_realA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1725\u001b[0m                                                     class_weight)\n\u001b[1;32m   1726\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"vNOAHU7nisf4"},"source":["while True:\n","    pass"],"execution_count":null,"outputs":[]}]}